# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KBiwVplTGRM2IMx4NgmfAc8tGdIOR23l
"""

# !pip install gradio transformers torch nltk
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import nltk
from nltk.tokenize import sent_tokenize

nltk.download('punkt', quiet=True)



device = ("cuda" if torch.cuda.is_available()
          else "mps" if torch.backends.mps.is_available()
          else "cpu")
print(f"device: {device}")

cache_dir = "_cache"


# "name to show": "name to load"
models = {
    # "dicta-il/dictalm2.0-instruct": "dicta-il/dictalm2.0-instruct",
    # "distilgpt2": "distilgpt2",
    # "gpt2": "gpt2",
    "meta-llama/Llama-2-7b-hf": "meta-llama/Llama-2-7b-hf",
    # "sshleifer/tiny-gpt2": "sshleifer/tiny-gpt2"
}

loaded_models = {name:
                     {"model": AutoModelForCausalLM.from_pretrained(models[name], torch_dtype=torch.bfloat16, cache_dir=cache_dir).to(device),
                      "tokenizer": AutoTokenizer.from_pretrained(models[name], padding=True, cache_dir=cache_dir)}
                 for name in models}


def generate_response(input_text, model_name, max_new_tokens=50, temperature=0.7, top_p=0.9, top_k=50):
    model_data = loaded_models[model_name]
    model = model_data["model"]
    tokenizer = model_data["tokenizer"]

    inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(input_text):].strip()

def create_paragraphs(bot_response, sentences_per_paragraph=2):
    sentences = sent_tokenize(bot_response)
    paragraphs = []
    current_paragraph = ""

    for i, sentence in enumerate(sentences, start=1):
        current_paragraph += " " + sentence
        if i % sentences_per_paragraph == 0:
            paragraphs.append(current_paragraph.strip())
            current_paragraph = ""

    if current_paragraph:
        paragraphs.append(current_paragraph.strip())

    formatted_paragraphs = "\n".join([f'<p style="text-align: right; direction: rtl;">{p}</p>' for p in paragraphs])
    return formatted_paragraphs

def chat(input_text, prompt, history, model_name, max_new_tokens, temperature, top_p, top_k, create_paragraphs_enabled):
    user_input = f'<div style="text-align: right; direction: rtl;">{input_text}</div>'
    response = generate_response(prompt + input_text, model_name, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p, top_k=top_k)

    if create_paragraphs_enabled:
        response = create_paragraphs(response)

    bot_response = f'<div style="text-align: right; direction: rtl;">{response}</div>'
    history.append((user_input, bot_response))

    return history, history


with gr.Blocks() as demo:
    gr.Markdown("# Multi-Model Language Chatbot", elem_id="title")

    chatbot = gr.Chatbot(elem_id="chatbot")

    with gr.Row():
        message = gr.Textbox(placeholder="your message...", label="user", elem_id="message", scale=3)
        prompt = gr.Textbox(placeholder="your prompt...", label="prompt", elem_id="message", scale=3)
        submit = gr.Button("send", scale=1)

    with gr.Row():
        model_dropdown = gr.Dropdown(choices=list(models.keys()), value=list(models.keys())[0], label="choose model")
        create_paragraphs_checkbox = gr.Checkbox(label="create paragraphs", value=False)

    with gr.Accordion("settings", open=False):
        with gr.Row():
            with gr.Column():
                max_new_tokens = gr.Slider(minimum=1, maximum=2000, value=50, step=1, label="max new tokens")
                temperature = gr.Slider(minimum=0.1, maximum=1.0, value=0.7, step=0.1, label="temperature")
            with gr.Column():
                top_p = gr.Slider(minimum=0.1, maximum=1.0, value=0.9, step=0.1, label="Top P")
                top_k = gr.Slider(minimum=1, maximum=100, value=50, step=1, label="Top K")

    submit.click(chat, inputs=[message, prompt, chatbot, model_dropdown, max_new_tokens, temperature, top_p, top_k, create_paragraphs_checkbox], outputs=[chatbot, chatbot])

    demo.css = """
        #message, #message * {
            text-align: right !important;
            direction: rtl !important;
        }

        #chatbot, #chatbot * {
            text-align: right !important;
            direction: rtl !important;
        }

        #title, .label {
            text-align: right !important;
        }
    """

print("Starting the server. This may take a few minutes as all models are being loaded...")
demo.launch()